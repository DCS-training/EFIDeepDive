---
title: "EFI Deep Dive"
author: "Lucia Michielin"
date: "9/13/2024"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Libraries Install and Load, include=FALSE }
#install.packages("tmap")
library(tmap)
library(tm)
library(sf)
library(RColorBrewer)
library(here)
library(tidyverse)
library(data.table)
```

## Introduction

This is an [R Markdown](%5Bhttp://rmarkdown.rstudio.com)](<http://rmarkdown.rstudio.com>)) Notebook. When you execute code within the notebook, the results appear beneath the code.

Try executing this chunk by clicking the \*Run\* button within the chunk or by placing your cursor inside it and pressing \*Ctrl+Shift+Enter\*.

```{r cars, echo=FALSE}
plot(cars)
```

Ok that is a standard dataset that does not mean much. The mtcars dataset is a built-in dataset in R that contains measurements on 11 different attributes for 32 different cars.

So let's see some thinking more interesting!

# Our Dataset

The Statistical Accounts of Scotland are a series of documentary publications, related in subject matter though published at different times, covering life in Scotland in the 18th and 19th.

The Old (or First) Statistical Account of Scotland was published between 1791 and 1799 by Sir John Sinclair of Ulbster. The New (or Second) Statistical Account of Scotland published under the auspices of the General Assembly of the Church of Scotland between 1834 and 1845. These first two Statistical Accounts of Scotland are unique records of life during the agricultural and industrial revolutions in Europe.

## Structure of the dataset

The original publication has been scanned and OCRed and each single record has been collected in a .txt file. The name of each file contain information about the document itself. For example StAS.2.15.91.P.Orkney.Cross_and_Burness

-   StAs.2.15.91 -\> Second Statistical Account

-   P -\> Parish (Contain information from the Parish)

-   Orkney -\> Area of interest (Scotland has been divided in 33 Areas)

-   Cross_and_Burness -\> Parish

We are going to see how to use this to extract information about all our text later but the first thing we need to do is to create a single dataframe (table) that will contain all the texts otherwise it will be very difficult to manage the data.

## Prepare the dataset

All our .txt files are in a directory named Account so I can write a function that will loop through each of the files extract the text and the tile of each file and put them all in a table.

Doing it manually would take a ridiculous amount of time but that is what computer are for so let's see what we can do.

1\. Create a new object that contain the path to our directory

```{r set directory}
text_files_dir <- "Accounts"
```

2.  Create an empty data.table that we are going to populate with the info we are going to extract

```{r create new table}
text_files <- list.files(text_files_dir, pattern = "\\.txt$", full.names = TRUE)#search for .txt

Scotdata <- data.table(title = character(), text = character())# create a table with two column one named title and one text
```

3.  Iterate through each text file We do this by using a forloop function

```{r populate table}
for (file in text_files) {
  # Specify the encoding (e.g., "latin1")
  text <- tolower(iconv(readLines(file, warn = FALSE), from = "latin1", to = "UTF-8", sub = ""))# tolower gets all text low cap 
  title <- gsub(".txt$", "", basename(file))# gsub extracts the pattern define so the tile of the files before .txt
  Scotdata <- rbindlist(list(Scotdata, data.table(title = title, text = paste(text, collapse = " "))))# bind them together
}
```

4.  look at the first 5 row of our file and save the table as a .csv so I do not have to do it every single time

```{r}
head(Scotdata)
write.csv(Scotdata, "text_data.csv", row.names = FALSE)
```

## Clean and format the data

### Fix some formatting issues

Fix the going to the next line issue. i.e. sub "- " with nothing "" There are a lot of formatting errors (next line, next paragraph) that we want to clean up

```{r}
ScotdataClean <- mutate_if(Scotdata, 
                           is.character, #apply the changes only if the data is a "character" type (e.g. text)
                           str_replace_all, 
                           pattern = "-[[:space:]]+","") #What I am searching for+ what I am subbing with 
```

### Extract More info from the dataset

To do the following steps we are using regex. Short for regular expression, a regex is a string of text that lets you create patterns that help match, locate, and manage text. Think find and replace in Word

1.  Extract area and parish from the title

-   P=Parish
-   C=Miscellanea
-   G=General Observations
-   A=Appendix
-   F=General
-   I=Index
-   M=Map

I want to be able to subset the dataset by those and I also want to have them both as a code as a description to do so I need to write a if else clause

```{r}
ScotdataClean$Type<- sub(".*(P|C|G|A|F|M|I)\\.(.*?)\\..*", "\\1", ScotdataClean$title)#This is selecting the P|C|G|A|F|M|I
ScotdataClean$TypeDescriptive<- ifelse(
  ScotdataClean$Type =="P", "Parish",ifelse(
    ScotdataClean$Type =="C","Miscellanea", ifelse(
      ScotdataClean$Type =="G","General Observations", ifelse(
        ScotdataClean$Type =="A", "Appendix", ifelse(
          ScotdataClean$Type =="F","General", ifelse(
            ScotdataClean$Type =="I", "Index","Map"))))))
```

2.  I want the first bit of the title as the RecordId of the document

```{r}
ScotdataClean$RecordID<- sub("^(StAS\\.\\d+\\.\\d+\\.\\d+).*","\\1",  ScotdataClean$title)
```

3.  I also want to extract the area that is the bit after p/c/g/a/f/m/i

```{r}
ScotdataClean$Area<- sub(".*(P|C|G|A|F|M|I)\\.(.*?)\\..*", "\\2", ScotdataClean$title)# //2 cause I want to select the second bit so after the letters
```

4.  Extract the Parish. I can do so by extracting the last bit up until the full stop

```{r}
ScotdataClean$Parish<- sub(".*\\.", "", ScotdataClean$title)
```

5.  Extract dates from the text I know all dates are cluster of 4 numbers starting with 1 (there are no recent dates)

```{r}
ScotdataClean$Year<-sapply(str_extract_all(ScotdataClean$text, "\\b1[1-9]{3}\\b"), function(matches) {
  if (length(matches) > 0) {
    paste(matches, collapse = ", ")
  } else {
    NA
  }
})
```

### Subset the dataset to only keep the text with information from the parishes

We will now start to look at what is inside but before starting our analysis we want to work only on the parish observations since a lot of the other documents are part of indexes or summaries

```{r}
Parish<-subset(ScotdataClean, Type =="P")
```

# Explore the dataset created

Create a Quanteda corpus of the 'text' column from our data set A corpus class object containing the original texts, document-level variables, document-level metadata, corpus-level metadata, and default settings for subsequent processing of the corpus. For quanteda \>= 2.0, this is a specially classed character vector.

```{r}
CorpusStat<-corpus(Parish$text)
```

## Summarise the content of the corpus

Print doc in position 5 of the corpus

```{r}
summary(CorpusStat, 5)
```

Check how many docs are in the corpus

```{r}
ndoc(CorpusStat) 
```

Check number of characters in the first 10 documents of the corpus

```{r}
nchar(CorpusStat[1:10])
```

Check number of tokens in the first 10 documents

```{r}
ntoken(CorpusStat[1:10]) 
```
